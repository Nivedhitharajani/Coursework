{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO66+NFVJ9Mlni/OqQukQHA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nivedhitharajani/Coursework/blob/main/AML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R3cExUOihq_2",
        "outputId": "1c4ac88d-4083-458f-c5c6-6e82acd5bca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'clip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-729985729.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'clip'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from tqdm import tqdm\n",
        "import clip\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "\n",
        "# === Setup ===\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
        "\n",
        "# === CoCoOp Prompt Learner ===\n",
        "class CoCoOpPromptLearner(nn.Module):\n",
        "    def __init__(self, ctx_len=8, embed_dim=512):\n",
        "        super().__init__()\n",
        "        self.ctx = nn.Parameter(torch.randn(ctx_len, embed_dim))\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, image_features):\n",
        "        image_cond = self.mlp(image_features).unsqueeze(1)\n",
        "        prompt = self.ctx.unsqueeze(0).expand(image_features.size(0), -1, -1)\n",
        "        return torch.cat([image_cond, prompt], dim=1)\n",
        "\n",
        "# === Dataset loader (subset)\n",
        "def load_datasets():\n",
        "    data_root = \"C:/Users/navee/Downloads/CLIP/data\"\n",
        "    paths = {\n",
        "        \"Caltech101\": os.path.join(data_root, \"caltech101\", \"caltech101\", \"101_ObjectCategories\"),\n",
        "        \"Food-101\": os.path.join(data_root, \"food-101\", \"images\"),\n",
        "        \"Stanford Cars\": os.path.join(data_root, \"stanford_cars\", \"train\"),\n",
        "        \"CIFAR-10\": os.path.join(data_root, \"cifar10_imagefolder\", \"train\"),\n",
        "        \"FGVC Aircraft\": os.path.join(data_root, \"fgvc_aircraft\")\n",
        "    }\n",
        "\n",
        "    datasets_map = {}\n",
        "    for name, path in paths.items():\n",
        "        ds = datasets.ImageFolder(path, transform=preprocess)\n",
        "        indices = list(range(len(ds)))\n",
        "        random.shuffle(indices)\n",
        "        subset = Subset(ds, indices[:300])\n",
        "        datasets_map[name] = subset\n",
        "    return datasets_map\n",
        "\n",
        "# === Evaluation Function\n",
        "def evaluate(model, subset, classnames):\n",
        "    loader = DataLoader(subset, batch_size=32, shuffle=False)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    confidences = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in classnames]).to(device)\n",
        "        text_features = F.normalize(clip_model.encode_text(text_inputs), dim=-1)\n",
        "\n",
        "        for images, labels in tqdm(loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            image_features = F.normalize(clip_model.encode_image(images), dim=-1)\n",
        "\n",
        "            prompts = model(image_features)\n",
        "            pooled = prompts.mean(dim=1)\n",
        "            logits = pooled @ text_features.T\n",
        "\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            preds = probs.argmax(dim=1)\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            confidences.extend(probs.max(dim=1).values.cpu().tolist())\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    return acc, confidences\n",
        "\n",
        "# === Main\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(42)\n",
        "    datasets_map = load_datasets()\n",
        "    model = CoCoOpPromptLearner(ctx_len=8).to(device)\n",
        "    model.load_state_dict(torch.load(\"cocoop_prompt_learned_fast.pth\", map_location=device))\n",
        "\n",
        "    results = {}\n",
        "    all_confidences = []\n",
        "\n",
        "    for name, subset in datasets_map.items():\n",
        "        print(f\"\\nüîç Evaluating {name}...\")\n",
        "        acc, confidences = evaluate(model, subset, subset.dataset.classes)\n",
        "        results[name] = acc\n",
        "        all_confidences.extend(confidences)\n",
        "\n",
        "    # === Log Results\n",
        "    with open(\"cocoop_prompt_eval_log.txt\", \"w\") as f:\n",
        "        f.write(\"Evaluation Results - CoCoOp Prompt Learner\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\")\n",
        "        for name, acc in results.items():\n",
        "            f.write(f\"{name}: {acc:.2f}%\\n\")\n",
        "\n",
        "    # === Bar Plot\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(results.keys(), results.values(), color=\"skyblue\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.title(\"CoCoOp Prompt Tuning Accuracy (Subset, Fast Mode)\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(axis=\"y\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"cocoop_prompt_eval_bar.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # === KDE Confidence Plot\n",
        "    if all_confidences:\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        sns.kdeplot(x=all_confidences, fill=True, color='green', alpha=0.4)\n",
        "        plt.title(\"Confidence Distribution (CoCoOp Prompt Tuning)\")\n",
        "        plt.xlabel(\"Confidence\")\n",
        "        plt.ylabel(\"Density\")\n",
        "        plt.grid()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"cocoop_prompt_confidence_kde.png\")\n",
        "        plt.show()"
      ]
    }
  ]
}